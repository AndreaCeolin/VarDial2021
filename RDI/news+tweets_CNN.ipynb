{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "news+tweets_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFsdW2NRb_RL"
      },
      "source": [
        "#Romanian Dialect Identification - VarDial2021 - News and Tweets CNN\n",
        "\n",
        "This notebook contains the CNN used by Team Phlyers for the RDI shared task at VarDial2020 and VarDial2021. The network is an adaptation of the CNN presented in Butnaru and Ionescu (2019).\n",
        "\n",
        "The first few blocks are needed to set up the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElNfPjqpepZ9",
        "outputId": "e1e398d6-49fd-4bb4-e6e2-8961bd0dd575"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCKCET-8fdjB",
        "outputId": "d2d5ae7a-c682-451d-9cff-24ea7561806a"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/RMI-VarDial2021\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/RMI-VarDial2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HAFhRAkfmQ4"
      },
      "source": [
        "The next block loads the data and performs preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEIXZLAPb_RO",
        "outputId": "91f7ef73-3d55-4eec-d28b-d78bb911156e"
      },
      "source": [
        "from io import open\n",
        "from collections import defaultdict\n",
        "import string \n",
        "import random\n",
        "\n",
        "'''\n",
        "The code has been adapted from:\n",
        "\n",
        "Convolutional Neural Networks Tutorial in PyTorch \n",
        "(https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/)\n",
        "\n",
        "NLP FROM SCRATCH: CLASSIFYING NAMES WITH A CHARACTER-LEVEL RNN\n",
        "by Sean Robertson\n",
        "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html\n",
        "'''\n",
        "\n",
        "##############################\n",
        "# Load the data and preprocess\n",
        "##############################\n",
        "\n",
        "training_categories = []\n",
        "training_sentences = []\n",
        "dev_categories = []\n",
        "dev_sentences = []\n",
        "label_list = ['RO', 'MD']\n",
        "\n",
        "# Load the news training data and the tweets development data and preprocess\n",
        "for line in open('data/train.txt', encoding='utf-8', errors='ignore'):\n",
        "    category = line.split('\\t')[-1].rstrip().strip('\\u202c')\n",
        "    training_categories.append(category)\n",
        "    sentence = line.split('\\t')[0].replace('$NE$', '').lower()\n",
        "    training_sentences.append(sentence)\n",
        "\n",
        "for line in open('data/dev.txt', encoding='utf-8', errors='ignore'):\n",
        "    category = line.split('\\t')[-1].rstrip().strip('\\u202c')\n",
        "    dev_categories.append(category)\n",
        "    sentence = line.split('\\t')[0].replace('$NE$', '').lower().replace('foto', '').replace('video','').replace('live','')\n",
        "    sentence = sentence.translate(sentence.maketrans('', '', string.punctuation+'|-0123456789”„…'))\n",
        "    sentence = ' '.join(sentence.split())\n",
        "    dev_sentences.append(sentence)\n",
        "\n",
        "# These are letters that appear more than 50 times in the corpus. The others are excluded.\n",
        "all_letters = 'cumaspnetfârşidvoljgzţăbîxwhșțkyкуинсайдертябгqхоéàпвылшǎцáзфьмжщчãöü̦̆ю̧ȋэç'\n",
        "\n",
        "# Map the characters into a list of indeces, that you use to create the tensors\n",
        "n_letters = len(all_letters)\n",
        "dic_letters = dict(zip(all_letters, range(1, n_letters+1)))\n",
        "\n",
        "# Use part of the tweets development dataset to create a tweets test set\n",
        "X_dev = dev_sentences[len(dev_sentences)//5:]\n",
        "y_dev = dev_categories[len(dev_sentences)//5:] \n",
        "X_test = dev_sentences[:len(dev_sentences)//5]\n",
        "y_test = dev_categories[:len(dev_sentences)//5] \n",
        "\n",
        "# Print size of the corpus\n",
        "print('Characters:', all_letters)\n",
        "print('# of characters:', len(all_letters))\n",
        "print('# of training sentences:', len(training_categories))\n",
        "print('# of training labels:', len(set(training_categories)))\n",
        "print('# of dev sentences:', len(X_dev))\n",
        "print('# of dev labels:', len(set(y_dev)))\n",
        "\n",
        "# Store the words that appear in each category\n",
        "X_dic = defaultdict(set)\n",
        "for sentence, label in zip(X_dev,y_dev):\n",
        "  X_dic[label].update(sentence.split())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Characters: cumaspnetfârşidvoljgzţăbîxwhșțkyкуинсайдертябгqхоéàпвылшǎцáзфьмжщчãöü̦̆ю̧ȋэç\n",
            "# of characters: 76\n",
            "# of training sentences: 33564\n",
            "# of training labels: 2\n",
            "# of dev sentences: 4190\n",
            "# of dev labels: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8HWI-qVxBem"
      },
      "source": [
        "The next block contains some data augmentation functions inspired by Wei and Zou (2019)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyUsqDcrwZ2K",
        "outputId": "25628d0f-367e-41f5-9489-1f92afe32856"
      },
      "source": [
        "##################\n",
        "#Data Augmentation\n",
        "#################\n",
        "\n",
        "augmented_labels = []\n",
        "augmented_sentences = []\n",
        "\n",
        "#This function adds 10 replicas of the tweets set with shuffled sentences\n",
        "def random_shuffle(): \n",
        "  for i in range(10):\n",
        "    for label, sentence in zip(y_dev, X_dev):\n",
        "      augmented_labels.append(label)\n",
        "      new_sentence = sentence.split()\n",
        "      random.shuffle(new_sentence)\n",
        "      augmented_sentences.append(' '.join(new_sentence))\n",
        "\n",
        "#This function adds 10 replicas of the tweets set with random insertion \n",
        "def random_insert():\n",
        "  for i in range(10):\n",
        "    for label, sentence in zip(y_dev, X_dev):\n",
        "      augmented_labels.append(label)\n",
        "      new_sentence = sentence.split()\n",
        "      a = random.choice(list(X_dic[label]))\n",
        "      new_sentence.insert(random.choice(range(len(new_sentence))), a)\n",
        "      augmented_sentences.append(' '.join(new_sentence))\n",
        "\n",
        "#This functions adds 10 replicas of the tweets set with random deletion \n",
        "def random_delete():\n",
        "  for i in range(10):\n",
        "    for label, sentence in zip(y_dev, X_dev):\n",
        "      augmented_labels.append(label)\n",
        "      new_sentence = sentence.split()\n",
        "      new_sentence.pop(random.choice(range(len(new_sentence))))\n",
        "      augmented_sentences.append(' '.join(new_sentence))\n",
        "\n",
        "#This part adds 10 replicas of the tweets set with random swap\n",
        "def random_swap():\n",
        "  for i in range(10):\n",
        "    for label, sentence in zip(y_dev, X_dev):\n",
        "      augmented_labels.append(label)\n",
        "      new_sentence = sentence.split()\n",
        "      if len(new_sentence)>1:\n",
        "        w1, w2 = random.sample(range(len(new_sentence)),2)\n",
        "        new_sentence[w1], new_sentence[w2] = new_sentence[w2], new_sentence[w1] \n",
        "      augmented_sentences.append(' '.join(new_sentence))\n",
        "\n",
        "#Call the function\n",
        "random_shuffle()\n",
        "\n",
        "y_dev, X_dev = y_dev + augmented_labels, X_dev + augmented_sentences\n",
        "\n",
        "# Print size of the development set\n",
        "print('# of dev sentences augmented:', len(X_dev))\n",
        "print('# of dev labels augmented:', len(set(y_dev)))\n",
        "print('# of test sentences:', len(X_test))\n",
        "print('# of test labels:', len(set(y_test)))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of dev sentences augmented: 46090\n",
            "# of dev labels augmented: 2\n",
            "# of test sentences: 1047\n",
            "# of test labels: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbTxb5N5wf2R"
      },
      "source": [
        "The next block contains some helper functions that are needed to transform our input data in Tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctq5J3emwgY8",
        "outputId": "c971f255-79cf-44b4-8dca-dc78038c8587"
      },
      "source": [
        "import torch\n",
        "\n",
        "###################\n",
        "# Helper functions\n",
        "###################\n",
        "\n",
        "# Get the letter index\n",
        "def letterToIndex(letter):\n",
        "    if letter in dic_letters:\n",
        "        return dic_letters[letter]\n",
        "    return 0\n",
        "\n",
        "#Turns a single line into a tensor\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(n_letters+1, 5000)\n",
        "    for li, letter in enumerate(line[:5000]):\n",
        "        tensor[letterToIndex(letter)][li] = 1 \n",
        "    return tensor\n",
        "\n",
        "#Turns a batch of lines into a batch of tensors\n",
        "def linesToTensors(lines):\n",
        "    tensor = torch.zeros(batch_size, n_letters+1, 5000)\n",
        "    for batch, line in enumerate(lines):\n",
        "      for li, letter in enumerate(line[:5000]):\n",
        "          tensor[batch][letterToIndex(letter)][li] = 1 \n",
        "    return tensor\n",
        "\n",
        "#Turns categories into tensors\n",
        "def categoriesToTensors(categories):\n",
        "  labels = torch.zeros(len(categories),dtype=torch.long)\n",
        "  for i, label in enumerate(categories):\n",
        "    labels[i] = label_list.index(label)\n",
        "  return labels\n",
        "\n",
        "#Gets category from tensor\n",
        "def categoryFromOutput(output):\n",
        "  return label_list[int(output)]\n",
        "\n",
        "#Check that the category index is correct\n",
        "tensor = categoriesToTensors(['RO'])\n",
        "print(tensor)\n",
        "print(categoryFromOutput(tensor), 'RO')\n",
        "\n",
        "tensor_2 = categoriesToTensors(['MD'])\n",
        "print(tensor_2)\n",
        "print(categoryFromOutput(tensor_2), 'MD')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0])\n",
            "RO RO\n",
            "tensor([1])\n",
            "MD MD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrFCwafIwwlP"
      },
      "source": [
        "Next, we define the hyper-parameters of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4oM9i-IwNYI"
      },
      "source": [
        "##################\n",
        "#Hyper-parameters\n",
        "##################\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "batch_size = 128"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ANPXYHGUGb8"
      },
      "source": [
        "This is a class based on the DataLoader class that we will use to load the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_9dXnPyZBs1"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class MyClass(Dataset):\n",
        "    def __init__(self, training, labels):\n",
        "        self.training = training\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.training)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.training[idx], self.labels[idx])\n",
        "\n",
        "training_dataset = MyClass(X_dev, y_dev)\n",
        "dev_dataset = MyClass(X_test, y_test)\n",
        "\n",
        "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hixbQPqpb_Rd"
      },
      "source": [
        "This is the CNN adapted from Butnaru and Ionescu (2019)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7GUYQzQb_Rh",
        "outputId": "825b398f-8787-435d-bc67-718487a8448d"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "######################################################################\n",
        "# Creating the Network\n",
        "# ====================\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv1d(77, 128, kernel_size=7), nn.Threshold(threshold=0.000001, value=0), nn.MaxPool1d(3, stride=3))\n",
        "        self.conv2 = nn.Sequential(nn.Conv1d(128, 128, kernel_size=7), nn.Threshold(threshold=0.000001, value=0), nn.MaxPool1d(3, stride=3))\n",
        "        self.conv3 = nn.Sequential(nn.Conv1d(128, 128, kernel_size=3), nn.Threshold(threshold=0.000001, value=0), nn.MaxPool1d(3, stride=3))\n",
        "        self.fc1 = nn.Sequential(nn.Linear(23424, 1000), nn.Threshold(threshold=0.000001, value=0) ,nn.Dropout())\n",
        "        #For these two layers they do not specify the size. This was fine-tuned by us.\n",
        "        self.fc2 = nn.Sequential(nn.Linear(1000, 1000), nn.Threshold(threshold=0.000001, value=0) ,nn.Dropout())\n",
        "        self.fc3 = nn.Linear(1000, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        return softmax(out)\n",
        "\n",
        "model = Net()\n",
        "model = model.to('cuda')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print('The CNN is ready.')\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The CNN is ready.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAaw2bsdaoR7"
      },
      "source": [
        "This is the model we trained for VarDial2020. No need to train it again this time, we will just load the model that we saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "15Ewl-dTaWnT",
        "outputId": "ad10189d-5785-4ef0-f913-2066ceda0c3e"
      },
      "source": [
        "'''\n",
        "total_step = len(training_sentences)\n",
        "loss_list = []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  predicted_labels = []\n",
        "  correct_labels = []\n",
        "\n",
        "  for i, (lines, categories) in enumerate(dataloader):\n",
        "    tensors = linesToTensors(lines)\n",
        "    tensors = tensors.to(\"cuda\")\n",
        "    label_tensors = categoriesToTensors(categories)\n",
        "    label_tensors = label_tensors.to(\"cuda\")\n",
        "\n",
        "    outputs = model(tensors)\n",
        "    outputs = outputs.to(\"cuda\")\n",
        "\n",
        "    loss = criterion(outputs, label_tensors)\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "    # Backprop and perform Adam optimisation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track the training accuracy\n",
        "    total = label_tensors.size(0)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    correct = (predicted == label_tensors).sum().item()\n",
        "    acc = correct / total\n",
        "\n",
        "    predicted_labels.extend([int(label) for label in predicted])\n",
        "    correct_labels.extend([int(label) for label in label_tensors])\n",
        "\n",
        "  print('Training. Epoch-', epoch, 'F-score:', f1_score(predicted_labels, correct_labels, average='macro'))\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'trained_cnn_model')\n",
        "  \n",
        "'''"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntotal_step = len(training_sentences)\\nloss_list = []\\n\\n\\nfor epoch in range(num_epochs):\\n\\n  predicted_labels = []\\n  correct_labels = []\\n\\n  for i, (lines, categories) in enumerate(dataloader):\\n    tensors = linesToTensors(lines)\\n    tensors = tensors.to(\"cuda\")\\n    label_tensors = categoriesToTensors(categories)\\n    label_tensors = label_tensors.to(\"cuda\")\\n\\n    outputs = model(tensors)\\n    outputs = outputs.to(\"cuda\")\\n\\n    loss = criterion(outputs, label_tensors)\\n    loss_list.append(loss.item())\\n\\n    # Backprop and perform Adam optimisation\\n    optimizer.zero_grad()\\n    loss.backward()\\n    optimizer.step()\\n\\n    # Track the training accuracy\\n    total = label_tensors.size(0)\\n    _, predicted = torch.max(outputs.data, 1)\\n    correct = (predicted == label_tensors).sum().item()\\n    acc = correct / total\\n\\n    predicted_labels.extend([int(label) for label in predicted])\\n    correct_labels.extend([int(label) for label in label_tensors])\\n\\n  print(\\'Training. Epoch-\\', epoch, \\'F-score:\\', f1_score(predicted_labels, correct_labels, average=\\'macro\\'))\\n\\n\\ntorch.save(model.state_dict(), \\'trained_cnn_model\\')\\n  \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi9yGjheUl1I"
      },
      "source": [
        "Here we take the trained model, and we fine-tune it on the Tweets development set.\n",
        "\n",
        "In the process, we plot the accuracy on the training and on the development data on a Matplotlib plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "98r5NiihUmrj",
        "outputId": "82b54776-c2c9-4c95-8d49-163b5cc8c601"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load the model\n",
        "model.load_state_dict(torch.load('trained_cnn_model'))\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "# Train the model on the development dataset, and evaluate it on the test dataset\n",
        "# Print trainining and evaluation accuracies\n",
        "\n",
        "total_step = len(X_dev)\n",
        "loss_list = []\n",
        "training_accuracy = [0]\n",
        "dev_accuracy = [0]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  predicted_labels = []\n",
        "  correct_labels = []\n",
        "\n",
        "  for i, (sentences, category) in enumerate(training_dataloader):\n",
        "    tensors = linesToTensors(sentences)\n",
        "    tensors = tensors.to(\"cuda\")\n",
        "    labels = categoriesToTensors(category)\n",
        "    labels = labels.to(\"cuda\")\n",
        "\n",
        "    outputs = model(tensors)\n",
        "    outputs = outputs.to(\"cuda\")\n",
        "\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "    # Backprop and perform Adam optimisation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track the training accuracy\n",
        "    total = labels.size(0)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    acc = correct / total\n",
        "\n",
        "    predicted_labels.extend([int(label) for label in predicted])\n",
        "    correct_labels.extend([int(label) for label in labels])\n",
        "\n",
        "  #Store training accuracy\n",
        "  training_accuracy.append(f1_score(predicted_labels, correct_labels, average='macro'))\n",
        "  print('Training. Epoch-', epoch, 'F-score:', f1_score(predicted_labels, correct_labels, average='macro'))\n",
        "  test = [(sentence, category) for sentence, category in zip(X_test,y_test)]\n",
        "\n",
        "  #Check validation accuracy\n",
        "  predicted = []\n",
        "  correct = []\n",
        "  total = len(test)\n",
        "  for i, (sentence, category) in enumerate(test):\n",
        "    tensor = torch.reshape(lineToTensor(sentence), (1, n_letters+1, 5000))\n",
        "    tensor = tensor.to(\"cuda\")\n",
        "    outputs = model(tensor)\n",
        "    outputs = outputs.to(\"cpu\")\n",
        "    label = Variable(torch.LongTensor([label_list.index(category)]))\n",
        "    _, prediction = torch.max(outputs.data, 1)\n",
        "    predicted.append(prediction)\n",
        "    correct.append(label)\n",
        "  dev_accuracy.append(f1_score(predicted, correct, average='macro'))\n",
        "  print('Development. Epoch-', epoch, 'F-score:', f1_score(predicted, correct, average='macro'))\n",
        "\n",
        "#Plot training and evaluation accuracy\n",
        "n_epochs = [iter for iter in range(len(training_accuracy))]\n",
        "plt.plot(n_epochs, training_accuracy)\n",
        "plt.plot(n_epochs, dev_accuracy)\n",
        "plt.title('Training and Validation F1 score')\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training. Epoch- 0 F-score: 0.755078284551767\n",
            "Development. Epoch- 0 F-score: 0.7445510735198438\n",
            "Training. Epoch- 1 F-score: 0.8775602516601917\n",
            "Development. Epoch- 1 F-score: 0.7502342122653194\n",
            "Training. Epoch- 2 F-score: 0.9123469301720166\n",
            "Development. Epoch- 2 F-score: 0.7373438362350109\n",
            "Training. Epoch- 3 F-score: 0.929666530781789\n",
            "Development. Epoch- 3 F-score: 0.7307647287075346\n",
            "Training. Epoch- 4 F-score: 0.9392228630896693\n",
            "Development. Epoch- 4 F-score: 0.7598553352025014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}