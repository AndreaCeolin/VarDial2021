{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Track3_submission.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1b_SgfS6OZs"
      },
      "source": [
        "#Uralic Language Identification Task - VarDial2021 - Part 7\n",
        "\n",
        "This notebook contains the code developed by Team Phlyers for Track 3 of the ULI shared task at VarDial2021.\n",
        "\n",
        "The first few blocks are needed to set up the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7xfXFhJ3wdE",
        "outputId": "eccba395-6c11-479c-de8e-f0582a219c95"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsfW8K9j3w2I",
        "outputId": "fdfd1f8e-eb12-44e1-d093-2e7e064758e6"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/ULI-VarDial2021"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/ULI-VarDial2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnoEdUll6UNW"
      },
      "source": [
        "This block loads the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3nXSyF11Ema"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# The corpus is stored in a dictionary in json format\n",
        "# Dictionary format: {category:{language:[list of texts]}}\n",
        "\n",
        "with open('data.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "# Dataset is in the format of a tuple (category, lang, sentence)\n",
        "sentences = []\n",
        "\n",
        "for key in data:\n",
        "  for lang in data[key]:\n",
        "    for sentence in data[key][lang]:\n",
        "      sentences.append((lang, sentence))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMumiC7g6n6O"
      },
      "source": [
        "We classify tweets using a MNB classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpvlZOcY6vei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f017abe-f8fd-4afa-d917-ff8da79a0e4a"
      },
      "source": [
        "X_train = [X for _,X in sentences]\n",
        "y_train = [y for y,_ in sentences]\n",
        " \n",
        "test_sentences = []\n",
        "for line in open('test.txt', encoding='utf-8', errors='ignore'):\n",
        "    test_sentences.append(' '.join(line.lower().translate(line.maketrans('', '', string.punctuation+'|-0123456789”„…'+'\\t')).replace('http', '').replace('www', '').split()))\n",
        "\n",
        "vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(5,5), min_df=0.00001, sublinear_tf=True)\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "model = MultinomialNB(alpha=0.00000001)\n",
        "model.fit(X_train, y_train)\n",
        "print('Model fitted')\n",
        "# Vectorize Evaluation\n",
        "X_test = vectorizer.transform(test_sentences)\n",
        "print('Test set vectorized.')\n",
        "# Predict\n",
        "ypred = model.predict(X_test)\n",
        "print('Predictions made.')\n",
        "\n",
        "with open('ULI-track-3-Phlyers.txt', 'w') as f:\n",
        "  for label in ypred:\n",
        "    f.write(label + '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model fitted\n",
            "Test set vectorized.\n",
            "Predictions made.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}