{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Track1_submission.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q30eO0xz8iNe"
      },
      "source": [
        "#Uralic Language Identification Task - VarDial2021 - Part 6\n",
        "\n",
        "This notebook contains the code developed by Team Phlyers for Track 1 and 2 of the ULI shared task at VarDial2021.\n",
        "\n",
        "The first few blocks are needed to set up the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q83fvp9rjTMV",
        "outputId": "4bd60a27-b75d-42ea-ef8b-c92529239e53"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiDSbwpWjZ9v",
        "outputId": "a1fde9ab-8b86-434c-e47c-d2687b16f11d"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/ULI-VarDial2021"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/ULI-VarDial2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwGIUwyo8re-"
      },
      "source": [
        "This block contains the two classifiers we decided to use for the task (a SVM classifier to distinguish between 'target' and 'non-target' languages, and a NB classifier to distinguish among 'target' languages)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvPpCSNzjXEY"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def svm(train, test):\n",
        "    # Vectorize training set\n",
        "    vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3,4), max_features=100000)\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    X_train = vectorizer.fit_transform([sentence for key,sentence in train])\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    y_train = [key for key,sentence in train]\n",
        "    print('Rows x: ' + str(X_train.shape[0]))\n",
        "    print('Columns x: ' + str(X_train.shape[1]))\n",
        "    print('Labels y: ' + str(len(y_train)))\n",
        "    # Train a Naive Bayes classifier\n",
        "    model = SGDClassifier(max_iter=7000)\n",
        "    #model = MultinomialNB(alpha=0.00000001)\n",
        "    model.fit(X_train, y_train)\n",
        "    # Vectorize Evaluation\n",
        "    X_test = vectorizer.transform(test)\n",
        "    X_test = scaler.transform(X_test)\n",
        "    # Predict\n",
        "    ypred = model.predict(X_test)\n",
        "    return list(ypred)\n",
        "\n",
        "def mnb(train, submission, alpha, range, min):\n",
        "    # Vectorize training set\n",
        "    vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(range), min_df=min, sublinear_tf=True)\n",
        "    X_train = vectorizer.fit_transform([sentence for key,sentence in train])\n",
        "    y_train = [key for key,sentence in train]\n",
        "    print('Rows x: ' + str(X_train.shape[0]))\n",
        "    print('Columns x: ' + str(X_train.shape[1]))\n",
        "    print('Labels y: ' + str(len(y_train)))\n",
        "    # Train a Naive Bayes classifier\n",
        "    model = MultinomialNB(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    print('Model fitted')\n",
        "    # Vectorize Evaluation\n",
        "    X_test = vectorizer.transform(submission)\n",
        "    print('Test set vectorized.')\n",
        "    # Predict\n",
        "    ypred = model.predict(X_test)\n",
        "    # Calculate F-score globally and print F-score per category\n",
        "    print('Predictions have been made.')\n",
        "    return list(ypred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cLjTRao9jS9"
      },
      "source": [
        "This block loads the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK9qKGVqvh5k",
        "outputId": "7eee0298-1f51-40e6-9bb2-7335a3a0ba0a"
      },
      "source": [
        "# The corpus is stored in a dictionary in json format\n",
        "# Dictionary format: {category:{language:[list of texts]}}\n",
        "with open('data.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "# Dataset is in the format of a tuple (category, lang, sentence)\n",
        "dataset = []\n",
        "\n",
        "for category in data:\n",
        "  for lang in data[category]:\n",
        "    for sentence in data[category][lang]:\n",
        "      dataset.append((category, lang, sentence))\n",
        "\n",
        "print(\"Length of the dataset:\")\n",
        "print(len(dataset))\n",
        "\n",
        "training = [(category, sentence) for category, _, sentence in dataset]\n",
        "\n",
        "test_sentences = []\n",
        "for line in open('test.txt', encoding='utf-8', errors='ignore'):\n",
        "    test_sentences.append(' '.join(line.lower().translate(line.maketrans('', '', string.punctuation+'|-0123456789”„…'+'\\t')).replace('http', '').replace('www', '').split()))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of the dataset:\n",
            "1391043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ccr0uQY-Gk1"
      },
      "source": [
        "This block runs the SVM on the data to single out 'target' languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrOGtYWf-Hmt",
        "outputId": "2e6568a0-10ad-405f-ec6c-53f7f8785f6d"
      },
      "source": [
        "y_eval_pred = svm(training, test_sentences)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rows x: 1391043\n",
            "Columns x: 100000\n",
            "Labels y: 1391043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOM_QT6B-Ur-"
      },
      "source": [
        "This block runs the MNB on the singled out sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79PBNA9G-Z9r",
        "outputId": "a1e0995c-1fc1-4858-ff47-76f039cae8ba"
      },
      "source": [
        "#extract (language,sentence) for all the target languages in the training dataset\n",
        "training_ural = [(language,sentence) for category, language, sentence in dataset if category == \"UR\"]\n",
        "\n",
        "\n",
        "\n",
        "#extract all sentences which are predicted to be ULI\n",
        "test_ural=[]\n",
        "\n",
        "for predicted, sentence in zip(y_eval_pred, test_sentences):\n",
        "    if predicted == \"UR\":\n",
        "        test_ural.append(sentence)\n",
        "\n",
        "\n",
        "#predict the languages for only ULI languages\n",
        "y_ural_predict = mnb(training_ural, test_ural, 0.0000001, (3,5), 0.000001)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rows x: 646043\n",
            "Columns x: 1677212\n",
            "Labels y: 646043\n",
            "Model fitted\n",
            "Test set vectorized.\n",
            "Predictions have been made.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_EjE90j_av3"
      },
      "source": [
        "Combine the predictions of the two classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rluWbhg__e6H"
      },
      "source": [
        "predicted_labels = []\n",
        "\n",
        "for prediction in y_eval_pred:\n",
        "  if prediction == \"UR\":\n",
        "    # if the prediction of the SVM classifier is that of a target language, retrieve the language label from the MNB predictions\n",
        "    predicted_labels.append(y_ural_predict.pop(0))\n",
        "  else:\n",
        "    # if the prediction of the SVM classifier is that of a non-target language, then predict 'NA'\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "with open('ULI-track-1-Phlyers.txt', 'w') as f:\n",
        "  for label in predicted_labels:\n",
        "    f.write(label + '\\n')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}