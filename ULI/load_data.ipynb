{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "load_data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAgMpiTSMPhU"
      },
      "source": [
        "#Uralic Language Identification Task - VarDial2021 - Part 1\"\n",
        "\n",
        "This notebook contains the code developed by Team Phlyers to extract the Wanca 2017 Corpus (Jauhianen et al. 2020) for the ULI shared task at VarDial2021.\n",
        "\n",
        "The first few blocks are needed to set up the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB-vFJZfUA0A",
        "outputId": "7cd09b13-340f-430d-e503-440b57ad5f60"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meZxyAmYUPpK",
        "outputId": "5155f3ce-ac82-4e9e-b3cb-e6a5e51784a6"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/ULI-VarDial2021"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/ULI-VarDial2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q3D8aVkND2U"
      },
      "source": [
        "This block loads the data and stores them in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuLIMmt3UUAV"
      },
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "import string\n",
        "import json\n",
        "\n",
        "###############\n",
        "#Load the data\n",
        "###############\n",
        "\n",
        "# Dictionary format: {category:{language:[list of texts]}}\n",
        "lang_dic=defaultdict(dict)\n",
        "\n",
        "for file in os.listdir('ULI_data'):\n",
        "    print(file, len(file))\n",
        "    # All files of length 7 contain the 'target' languages\n",
        "    if len(file) == 7:\n",
        "      # We retrieve the name of the language\n",
        "        name = file[:3]\n",
        "        sentences=[]\n",
        "        for line in open('ULI_data/' + file, encoding='utf-8', errors='ignore'):\n",
        "            sentences.append(' '.join(line.lower().translate(line.maketrans('', '', string.punctuation+'|-0123456789”„…'+'\\t')).split()[:-1]))\n",
        "        # 'UR' is the label we attribute to the target language category\n",
        "        lang_dic['UR'][name] = sentences\n",
        "    # The range covers all the files containing 'non-target' languages\n",
        "    elif len(file) in range(29,42):\n",
        "        sentences=[]\n",
        "        # We retrieve the name of the language\n",
        "        name = file[:3]\n",
        "        i = 0\n",
        "        for line in open('ULI_data/' + file, encoding='utf-8', errors='ignore'):\n",
        "            if i<5000:\n",
        "                sentences.append(' '.join(line.lower().translate(line.maketrans('', '', string.punctuation+'|-0123456789”„…'+'\\t')).replace('http', '').replace('www', '').split()))\n",
        "                i+=1\n",
        "        # 'Na' is the label we attribute to the non-target language category\n",
        "        lang_dic['Na'][name] = sentences\n",
        "\n",
        "print('Categories:')\n",
        "print(lang_dic.keys())\n",
        "print('# of Target langs:')\n",
        "print(len(lang_dic['UR']))\n",
        "print('# of Non-target langs:')\n",
        "print(len(lang_dic['Na']))\n",
        "print('Target langs:')\n",
        "print(lang_dic['UR'].keys())\n",
        "print('Non-Target langs:')\n",
        "print(lang_dic['Na'].keys())\n",
        "print(\"Examples:\")\n",
        "print(lang_dic['UR']['nio'][:5])\n",
        "print(lang_dic['UR']['sme'][:5])\n",
        "print(lang_dic['Na']['ceb'][:5])\n",
        "print(lang_dic['Na']['fin'][:5])\n",
        "\n",
        "\n",
        "# Dump the dictionary containing the corpus into a json file\n",
        "with open('data.json', 'w') as fp:\n",
        "    json.dump(lang_dic, fp)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}